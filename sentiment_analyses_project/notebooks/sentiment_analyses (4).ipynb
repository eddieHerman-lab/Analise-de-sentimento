Notebook JSON Format

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_markdown"
      },
      "source": [
        "# Análise de Textos em PDF\n",
        "\n",
        "Este notebook demonstra várias técnicas de análise de texto, incluindo:\n",
        "- Processamento de PDFs\n",
        "- Análise de sentimentos\n",
        "- Nuvem de palavras\n",
        "- Modelagem de tópicos\n",
        "- Análise de redes de palavras\n",
        "- Clustering de sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "import PyPDF2\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel, CoherenceModel\n",
        "import networkx as nx\n",
        "from collections import defaultdict, Counter\n",
        "import community.community_louvain as community_louvain\n",
        "import plotly.graph_objs as go\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "processing_functions_md"
      },
      "source": [
        "## Funções de Processamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "processing_functions"
      },
      "source": [
        "def process_pdf(file_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PdfReader(file)\n",
        "            for i, page in enumerate(reader.pages):\n",
        "                try:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text and page_text.strip():\n",
        "                        text += page_text + \"\\n\"\n",
        "                    else:\n",
        "                        print(f\"A página {i + 1} está vazia ou não pode ser lida.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro ao processar a página {i + 1}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao abrir o arquivo PDF: {e}\")\n",
        "    return text\n",
        "\n",
        "def clean_text(text, use_lemmatization=True):\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        print(\"Texto vazio recebido para limpeza.\")\n",
        "        return []\n",
        "\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\[\\d+\\]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = text.lower()\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words_pt = set(stopwords.words('portuguese'))\n",
        "    stop_words_en = set(stopwords.words('english'))\n",
        "    stop_words = stop_words_pt.union(stop_words_en)\n",
        "    words = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
        "\n",
        "    if use_lemmatization:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        words = [lemmatizer.lemmatize(token) for token in words]\n",
        "\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "analysis_functions_md"
      },
      "source": [
        "## Funções de Análise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "analysis_functions"
      },
      "source": [
        "def word_frequency(words):\n",
        "    df = pd.DataFrame(words, columns=['word'])\n",
        "    freq = df.groupby('word').size().reset_index(name='frequency')\n",
        "    return freq.sort_values('frequency', ascending=False)\n",
        "\n",
        "def sentiment_analysis(words, vader=True):\n",
        "    text = ' '.join(words)\n",
        "    if vader:\n",
        "        vader_analyzer = SentimentIntensityAnalyzer()\n",
        "        scores = vader_analyzer.polarity_scores(text)\n",
        "        compound = scores['compound']\n",
        "        return 'Positivo' if compound >= 0.05 else 'Negativo' if compound <= -0.05 else 'Neutro'\n",
        "    blob = TextBlob(text)\n",
        "    return 'Positivo' if blob.sentiment.polarity > 0 else 'Negativo' if blob.sentiment.polarity < 0 else 'Neutro'\n",
        "\n",
        "def generate_wordcloud(words):\n",
        "    text = ' '.join(words)\n",
        "    wordcloud = WordCloud(width=800, height=600, background_color='white').generate(text)\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Nuvem de Palavras')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example_usage_md"
      },
      "source": [
        "## Exemplo de Uso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "example_usage"
      },
      "source": [
        "# Análise de exemplo\n",
        "file_path = \"seu_arquivo.pdf\"  # Ajuste o caminho conforme necessário\n",
        "text = process_pdf(file_path)\n",
        "clean_words = clean_text(text)\n",
        "\n",
        "# Frequência de palavras\n",
        "freq_df = word_frequency(clean_words)\n",
        "print(\"Top 10 palavras mais frequentes:\")\n",
        "print(freq_df.head(10))\n",
        "\n",
        "# Nuvem de palavras\n",
        "generate_wordcloud(clean_words)\n",
        "\n",
        "# Análise de sentimentos\n",
        "sentiment = sentiment_analysis(clean_words)\n",
        "print(f\"\\nSentimento do texto: {sentiment}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
